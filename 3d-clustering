import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
import os
import re
import random


# Set number of threads to avoid a known issue with KMeans on Windows with MKL
os.environ['OMP_NUM_THREADS'] = '1'

def preprocess_text(text):
    stemmer = PorterStemmer()
    text = text.lower()
    text = re.sub(r'\W', ' ', text)
    words = text.split()
    words = [stemmer.stem(word) for word in words]
    return ' '.join(words)

def fit_vectorizer(corpus):
    stop_words = set(stopwords.words('english'))
    return TfidfVectorizer(stop_words=list(stop_words), ngram_range=(1, 2)).fit(corpus)

def calculate_similarity(vectorizer, phrase1, phrase2):
    phrase1, phrase2 = preprocess_text(phrase1), preprocess_text(phrase2)
    vectors = vectorizer.transform([phrase1, phrase2]).toarray()
    return cosine_similarity(vectors)[0, 1]

def compute_keyword_similarity_scores(keyword, topics, pages, vectorizer):
    topic_scores = {topic: calculate_similarity(vectorizer, keyword, topic) for topic in topics}
    page_scores = {page: calculate_similarity(vectorizer, keyword, page) for page in pages}
    most_similar_topic = max(topic_scores, key=topic_scores.get)
    most_similar_page = max(page_scores, key=page_scores.get)
    highest_topic_similarity_score = topic_scores[most_similar_topic]
    highest_page_similarity_score = page_scores[most_similar_page]
    return highest_topic_similarity_score, highest_page_similarity_score, most_similar_topic, most_similar_page

def main():
    unique_pages = ['Home', 'About', 'Website Development', 'SEO Services', 'Blog', 'Contact']
    primary_topic_search_queries = ['SEO Basics', 'Digital Marketing', 'Content Strategy', 'PPC Campaigns', 'Web Analytics', 'Customer Engagement', 'SEO Services']
    vectorizer = fit_vectorizer(unique_pages + primary_topic_search_queries)

    manually_inputted_keywords = [
        'SEO', 'marketing', 'analytics', 'web design', 'content', 'engagement',
        'social media', 'PPC', 'email marketing', 'branding', 'conversion rate',
        'customer journey', 'user experience', 'backlinks', 'SERP', 'digital advertising',
        'keyword research', 'landing page', 'organic traffic', 'remarketing', 'A/B testing',
        'click-through rate', 'copywriting', 'lead generation', 'mobile optimization',
        'inbound marketing', 'affiliate marketing', 'content management', 'influencer marketing',
        'sales funnel', 'search query', 'target audience', 'viral marketing', 'bounce rate',
        'domain authority', 'meta description', 'page speed', 'ROI', 'social proof',
        'call to action', 'CMS', 'CRM', 'customer segmentation', 'KPI', 'link building',
        'online reputation', 'SEO audit', 'user interface', 'data-driven marketing', 'seo basics',
    ]

    data = []
    for keyword in manually_inputted_keywords:
        highest_topic_similarity_score, highest_page_similarity_score, most_similar_topic, most_similar_page = compute_keyword_similarity_scores(
            keyword, primary_topic_search_queries, unique_pages, vectorizer)
        data.append({
            'keyword': keyword,
            'page': most_similar_page,
            'topic_similarity_score': highest_topic_similarity_score,
            'page_similarity_score': highest_page_similarity_score,
            'primary_topic': most_similar_topic
        })
    
    plot_data(data, unique_pages, primary_topic_search_queries)

def plot_data(data, unique_pages, primary_topic_search_queries):
    keywords = [d['keyword'] for d in data]
    pages = [d['page'] for d in data]
    topic_similarity_scores = [d['topic_similarity_score'] for d in data]
    page_similarity_scores = [d['page_similarity_score'] for d in data]

    pages_idx = [unique_pages.index(page) for page in pages]
    primary_topics_idx = [primary_topic_search_queries.index(d['primary_topic']) for d in data]  # Fixed this line
    
    # Adding jitter: random noise to separate overlapping points
    jitter = 0.2
    pages_idx = [x + random.uniform(-jitter, jitter) for x in pages_idx]
    topic_similarity_scores = [x + random.uniform(-jitter, jitter) for x in topic_similarity_scores]
    
    
    # 3D Plot
    fig = plt.figure(figsize=(30, 20))
    ax = fig.add_subplot(111, projection='3d')
    sc = ax.scatter(pages_idx, primary_topics_idx, topic_similarity_scores, c=topic_similarity_scores, cmap='viridis')

    for i, keyword in enumerate(keywords):
        ax.text(pages_idx[i], primary_topics_idx[i], topic_similarity_scores[i], keyword)

    ax.set_xlabel('Pages')
    ax.set_ylabel('Primary Topics')
    ax.set_zlabel('Topic Similarity Scores')

    plt.colorbar(sc)
    plt.show()

    # 2D Plot
    plt.figure(figsize=(10, 10))
    scatter = plt.scatter(pages_idx, primary_topics_idx, c=topic_similarity_scores, cmap='viridis')

    for i, keyword in enumerate(keywords):
        plt.annotate(keyword, (pages_idx[i], primary_topics_idx[i]))

    plt.xlabel('Pages')
    plt.ylabel('Primary Topics')
    plt.colorbar(scatter)
    plt.show()

if __name__ == "__main__":
    main()
